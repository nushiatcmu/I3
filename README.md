# I3
# Feathr Movieâ€‘Streaming Feature Store Demo

A handsâ€‘on, endâ€‘toâ€‘end example that shows how to use **[Feathr](https://github.com/linkedin/feathr)** to build leakageâ€‘free, productionâ€‘ready features for a movieâ€‘streaming churnâ€‘prediction model.

The repo demonstrates:

* Declarative feature definitions in Python
* Automatic pointâ€‘inâ€‘time joins for offline training datasets
* Turnâ€‘key online serving backed by **Redis**
* CI/CD hooks (GitHub Actions) and observability with Prometheus/Grafana
* Cloudâ€‘agnostic deploymentâ€”flip a single flag to run the same code on local Spark, Databricks, EMR, or Snowflake

---

## ğŸš€ QuickÂ Start

```bash
# 1. Clone the repo
$ git clone https://github.com/<yourâ€‘org>/feathrâ€‘movieâ€‘streamingâ€‘demo.git
$ cd feathrâ€‘movieâ€‘streamingâ€‘demo

# 2. Launch the local sandbox (Spark + Redis + Prometheus)
$ docker compose up -d

# 3. Open the Feathr web console
$ open http://localhost:3000

# 4. Run the demo materialisation job
$ poetry run python src/materialise.py  # or `pip install -r requirements.txt` then `python ...`
```

> **Note**Â : the first run pulls Docker images (~1â€¯GB) and builds Spark; subsequent runs are much faster.

---

## ğŸ—‚ï¸Â RepositoryÂ Layout

```
.
â”œâ”€â”€ data/                      # âœ¨ Sample parquet datasets (watch_events, users, movies)
â”œâ”€â”€ src/                       # ğŸ Feature definitions & utility scripts
â”‚Â Â  â”œâ”€â”€ features.py            #   â€“ Declarative feature specs
â”‚Â Â  â”œâ”€â”€ materialise.py         #   â€“ Offline & online materialisation entryâ€‘point
â”‚Â Â  â””â”€â”€ __init__.py
â”œâ”€â”€ configs/
â”‚Â Â  â””â”€â”€ feathr_config.yaml     # âš™ï¸  Infra config (swap clusters/stores here)
â”œâ”€â”€ docker-compose.yaml        # ğŸ³ Local Spark, Redis, Prometheus, Grafana
â”œâ”€â”€ notebooks/
â”‚Â Â  â””â”€â”€ churn_feature_demo.ipynb # ğŸ““ Interactive walkthrough
â”œâ”€â”€ .github/workflows/ci.yml   # ğŸ¤– Lint + unit tests on every PR
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml             # (optional) Poetry config
â”œâ”€â”€ docs/
â”‚Â Â  â””â”€â”€ feathr_demo_architecture.png
â””â”€â”€ README.md
```

---

# ğŸ“„ FileÂ Contents

Below are the complete source files so you can copyâ€‘paste or scaffold your own repo quickly.

---

## `src/features.py`
```python
"""Declarative feature and anchor definitions for the movieâ€‘streaming demo."""
from feathr import (
    HdfsSource,
    Feature,
    FeatureAnchor,
    TypedKey,
    ValueType,
)

# ---------------------------------------------------------------------------
# 1. Data sources
# ---------------------------------------------------------------------------
watch_src = HdfsSource(name="watch_events", path="data/watch_events.parquet")
users_src = HdfsSource(name="users", path="data/users.parquet")

# ---------------------------------------------------------------------------
# 2. Entity keys
# ---------------------------------------------------------------------------
user_key = TypedKey(
    key_column="user_id",
    key_column_type=ValueType.INT32,
    description="User identifier",
)

# ---------------------------------------------------------------------------
# 3. Feature specs
# ---------------------------------------------------------------------------
f_watch_30d = Feature(
    name="watch_time_30d",
    key=user_key,
    feature_type="numeric",
    transform_expr="SUM(watch_time)",
    aggregation_interval="30d",
    window="30d",
)

f_lifetime = Feature(
    name="lifetime_days",
    key=user_key,
    transform_expr="DATEDIFF('day', CURRENT_DATE, signup_date)",
)

# ---------------------------------------------------------------------------
# 4. Anchors
# ---------------------------------------------------------------------------
anchor_watch = FeatureAnchor(
    name="watch_anchor",
    source=watch_src,
    features=[f_watch_30d],
)

anchor_users = FeatureAnchor(
    name="user_anchor",
    source=users_src,
    features=[f_lifetime],
)
```

---

## `src/materialise.py`
```python
"""Entryâ€‘point script to register and materialise features offline & online."""
from datetime import datetime
from feathr import FeathrClient, ObservationSettings

from features import (
    anchor_watch,
    anchor_users,
    f_watch_30d,
    f_lifetime,
)


def main() -> None:
    client = FeathrClient(config_path="configs/feathr_config.yaml")

    # ---------------------------------------------------------------------
    # 1. Register anchors & features in the registry
    # ---------------------------------------------------------------------
    client.register_features(anchor_watch, anchor_users)

    # ---------------------------------------------------------------------
    # 2. Backfill offline features (Parquet/Delta)
    # ---------------------------------------------------------------------
    client.materialize_features(
        features=[f_watch_30d, f_lifetime],
        start_time="2024-01-01",
        end_time=datetime.utcnow().strftime("%Y-%m-%d"),
    )

    # ---------------------------------------------------------------------
    # 3. Push latest feature values to the online store (Redis)
    # ---------------------------------------------------------------------
    client.materialize_features_online(features=[f_watch_30d, f_lifetime])

    # ---------------------------------------------------------------------
    # 4. Example: fetch pointâ€‘inâ€‘time training dataset
    # ---------------------------------------------------------------------
    obs = ObservationSettings(
        obs_path="data/labels.parquet",
        timestamp_column="ts",
    )
    training_df = client.get_offline_features(
        observation_settings=obs,
        features=[f_watch_30d, f_lifetime],
    )
    print("Training dataset rows:", training_df.count())

    # ---------------------------------------------------------------------
    # 5. Example: fetch online features for inference
    # ---------------------------------------------------------------------
    online_resp = client.get_online_features(
        feature_names=["watch_time_30d", "lifetime_days"],
        key={"user_id": 42},
    )
    print("Online lookup:", online_resp)


if __name__ == "__main__":
    main()
```

---

## `configs/feathr_config.yaml`
```yaml
project: movie_streaming_demo
spark_cluster: local[*]            # or "databricks", "emr", "snowflake"

offline_store:
  type: parquet
  location: data/feathr_offline

online_store:
  type: redis
  host: redis
  port: 6379

registry:
  type: file
  path: .feathr/registry.db
```

---

## `docker-compose.yaml`
```yaml
version: "3.9"
services:
  spark:
    image: bitnami/spark:3.4.1
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
    ports:
      - "7077:7077"   # Spark master
      - "8080:8080"   # Spark UI
    volumes:
      - ./:/workspace

  redis:
    image: redis:7
    ports:
      - "6379:6379"

  prometheus:
    image: prom/prometheus:v2.49.1
    ports:
      - "9090:9090"
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml:ro

  grafana:
    image: grafana/grafana:10.2.3
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      - prometheus

  feathr-ui:
    image: linkedin/feathr-ui:latest
    ports:
      - "3000:3000"
    environment:
      - FEATHR_BACKEND_URL=http://localhost:8080
```

---

## `requirements.txt`
```
feathr>=0.9.0
pyspark>=3.4.0
redis>=5.0.0
prometheus-client>=0.18.0
black>=23.10.0
pytest>=7.4.0
```
---

## ğŸ–¨ï¸Â Sample Output

Below is a **sample (fake) console transcript** from the first run of `python src/materialise.py` in the Docker sandbox. Feel free to include it in docs so newcomers know what to expect.

```text
$ poetry run python src/materialise.py
[2025â€‘04â€‘07 14:20:12]  INFO  feathr.client: Using config configs/feathr_config.yaml
[2025â€‘04â€‘07 14:20:12]  INFO  feathr.client: Registering 2 feature anchors and 2 featuresâ€¦
[2025â€‘04â€‘07 14:20:12]  INFO  feathr.registry: Wrote registry to .feathr/registry.db

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Spark Job 1/3  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Stage 0: Parse feature plan                                     (  1  +   0  )   00:00 âœ“
Stage 1: Scan data/watch_events.parquet                         ( 16  +   0  )   00:11 âœ“
Stage 2: Aggregate watch_time_30d                               ( 16  +   0  )   01:43 âœ“
Stage 3: Write Parquet offline sink                             (  1  +   0  )   00:08 âœ“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[2025â€‘04â€‘07 14:22:15]  INFO  feathr.client: âœ”ï¸ Offline materialisation finished (watch_time_30d)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Spark Job 2/3  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Stage 0: Scan data/users.parquet                               (  8  +   0  )   00:04 âœ“
Stage 1: Compute lifetime_days                                 (  8  +   0  )   00:00 âœ“
Stage 2: Write Parquet offline sink                            (  1  +   0  )   00:02 âœ“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[2025â€‘04â€‘07 14:22:24]  INFO  feathr.client: âœ”ï¸ Offline materialisation finished (lifetime_days)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Spark Job 3/3  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Stage 0: Stream features to Redis (hash key pattern: user_id:*) (  4  +   0  )   00:22 âœ“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[2025â€‘04â€‘07 14:22:48]  INFO  feathr.client: âœ”ï¸ Online materialisation finished (2 features, 4.2â€¯M rows)

[2025â€‘04â€‘07 14:22:48]  INFO  feathr.client: Fetching pointâ€‘inâ€‘time training datasetâ€¦
[2025â€‘04â€‘07 14:22:53]  INFO  feathr.client: Training dataset rows: 105â€¯328

[2025â€‘04â€‘07 14:22:53]  INFO  feathr.client: Example online lookup for user_id=42
Online lookup: {'watch_time_30d': 17432.0, 'lifetime_days': 386}

Total wallâ€‘clock time: 2â€¯minâ€¯41â€¯s
```

### NextÂ Steps

* Drop your sample parquet files into `data/` (or mount an S3 bucket) and run `python src/materialise.py`.
* Explore feature lineage in the Feathr web console at `http://localhost:3000`.
* Open Grafana (`http://localhost:3001`, admin/admin) to view freshness & nullâ€‘rate dashboards.
* Swap `spark_cluster` to `databricks` and point `online_store` at a managed Redis to move to the cloud.









